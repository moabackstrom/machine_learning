{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d00cfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f6cdc5",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30b8ecb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ReLu activation function\n",
    "def ReLu_forward(x):\n",
    "    return np.where(x > 0, x, 0)\n",
    "\n",
    "# The partial differential of the ReLu activation function\n",
    "def ReLu_diff(x):\n",
    "    return np.where(x > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e1f49a",
   "metadata": {},
   "source": [
    "## Simple neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e365163c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_network(object):\n",
    "    def __init__(self): \n",
    "        # The weights for the 3 layers, the first column is the bias\n",
    "        self.w1 = np.array([[-0.1, 0.7],\n",
    "                            [0.3,-0.2]])\n",
    "        \n",
    "        self.w2 = np.array([[-0.2, 1.1, 0.2],\n",
    "                            [-0.1, 0.5, -0.3]])\n",
    "        \n",
    "        self.w3 = np.array([[0.1, 0.5, 2.3]])\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Forward propagation of the network, input -> output\n",
    "        # First layer\n",
    "        bias_in  = np.array([[1]]) # The bias wegiths are multiplied by 1\n",
    "        self.x_b = np.vstack([bias_in, x]) # add the bias term to the input vector\n",
    "        self.b1 = np.matmul(self.w1, self.x_b)  # Calculate the sum of the input times the weights\n",
    "        a1 = ReLu_forward(self.b1) # the activation funciton \n",
    "        \n",
    "        # Second layer \n",
    "        self.a1_b = np.vstack([bias_in, a1]) # add bisas term to the vector \n",
    "        self.b2 = np.matmul(self.w2, self.a1_b) # Calculate the sum of the outpu from previus layer times the weights\n",
    "        a2 = ReLu_forward(self.b2) # the activation funciton \n",
    "\n",
    "        self.a2_b = np.vstack([bias_in, a2]) # add bisas term to the vector\n",
    "        y_hat = np.matmul(self.w3, self.a2_b) # Calculate the sum of the outpu from previus layer times the weights\n",
    "        # No activation funciton on the last layer\n",
    "        return y_hat\n",
    "        \n",
    "    def backward(self, part_L_y_hat):\n",
    "        # Backpropagation to calculate all the gradients. \n",
    "        # Third layer\n",
    "        delta_3 = part_L_y_hat \n",
    "        grad_w3 = delta_3*self.a2_b.T \n",
    "        \n",
    "        # Second layer\n",
    "        delta_2 = np.matmul(self.w3[:,1:].T, delta_3) * ReLu_diff(self.b2)\n",
    "        grad_w2 = np.matmul(delta_2,self.a1_b.T)\n",
    "        \n",
    "        # First layer\n",
    "        delta_1 = np.matmul(self.w2[:,1:].T, delta_2) * ReLu_diff(self.b1)\n",
    "        grad_w1 = np.matmul(delta_1,self.x_b.T)\n",
    "        \n",
    "        return grad_w1, grad_w2, grad_w3\n",
    "    \n",
    "    def update_weights(self, g1, g2, g3, gamma):\n",
    "        # Use the gradients to update the weights \n",
    "        self.w1 = self.w1 - gamma*g1\n",
    "        self.w2 = self.w2 - gamma*g2\n",
    "        self.w3 = self.w3 - gamma*g3\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991d3057",
   "metadata": {},
   "source": [
    "## Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "58426348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction by network: 1.9799999999999998\n"
     ]
    }
   ],
   "source": [
    "net = Simple_network()\n",
    "x = np.array([[2]])\n",
    "y_hat = net.forward(x)\n",
    "print('Prediction by network:',  y_hat[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6457247b",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "164c41df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: 7\n",
      "Loss: 12.600200000000003\n"
     ]
    }
   ],
   "source": [
    "# The ground truth\n",
    "y = x**2 + 3 \n",
    "print('Ground truth:', y[0,0])\n",
    "\n",
    "# The loss function (regression)\n",
    "L = 1/2*(y-y_hat)**2\n",
    "print('Loss:', L[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267ecc73",
   "metadata": {},
   "source": [
    "## Backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "700bd9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradients layer 1:\n",
      "[[ -8.534 -17.068]\n",
      " [  0.      0.   ]]\n",
      "\n",
      "Gradients layer 2:\n",
      "[[ -2.51    -3.263    0.    ]\n",
      " [-11.546  -15.0098   0.    ]]\n",
      "\n",
      "Gradients layer 3:\n",
      "[[-5.02   -6.1746 -2.761 ]]\n",
      "\n",
      "Updated prediciton 2.1435048475206897\n",
      "Loss: 11.792772583027517\n"
     ]
    }
   ],
   "source": [
    "# Partial derivative of the loss with respect to y_hat\n",
    "part_L_y_hat = -(y-y_hat)\n",
    "\n",
    "# Calculate all gradients in the network\n",
    "g1, g2, g3 = net.backward(part_L_y_hat)\n",
    "print('Gradients layer 1:')\n",
    "print(g1)\n",
    "print('')\n",
    "print('Gradients layer 2:')\n",
    "print(g2)\n",
    "print('')\n",
    "print('Gradients layer 3:')\n",
    "print(g3)\n",
    "print('')\n",
    "\n",
    "net.update_weights(g1, g2, g3, 0.001)\n",
    "\n",
    "y_hat = net.forward(x)\n",
    "print('Updated prediciton', y_hat[0,0])\n",
    "\n",
    "# The loss function (regression)\n",
    "L = 1/2*(y-y_hat)**2\n",
    "print('Loss:', L[0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6525cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
